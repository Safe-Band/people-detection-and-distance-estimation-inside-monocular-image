
@online{state_of_the_art_crowd_counting,
  author    = {github},
  title     = {Awesome Crowding},
  year      = {2021},
  url       = {https://github.com/gjy3035/Awesome-Crowd-Counting/blob/master/src/Perspective_Map.md},
  urldate   = {2025-02-28}
}

@article{state_of_the_art_datasets,
author = {Khan, Muhammad Asif and Menouar, Hamid and Ridha, Hamila},
year = {2022},
month = {11},
pages = {104597},
title = {Revisiting crowd counting: State-of-the-art, trends, and future perspectives},
volume = {129},
journal = {Image and Vision Computing},
doi = {10.1016/j.imavis.2022.104597}
}

@misc{khanam2024yolov11overviewkeyarchitectural,
      title={YOLOv11: An Overview of the Key Architectural Enhancements}, 
      author={Rahima Khanam and Muhammad Hussain},
      year={2024},
      eprint={2410.17725},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.17725}, 
}

@article{sindagi2020jhu-crowd++,
title={JHU-CROWD++: Large-Scale Crowd Counting Dataset and A Benchmark Method},
author={Sindagi, Vishwanath A and Yasarla, Rajeev and Patel, Vishal M},
journal={Technical Report},
year={2020}
}


@article{gao2020nwpu,
title={NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization},
author={Wang, Qi and Gao, Junyu and Lin, Wei and Li, Xuelong},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi={10.1109/TPAMI.2020.3013269},
year={2020}
}

@inbook{idress2018ucfqnrf,
author = {Idrees, Haroon and Tayyab, Muhmmad and Athrey, Kishan and Zhang, Dong and Al-ma'adeed, Somaya and Rajpoot, Nasir and Shah, Mubarak},
year = {2018},
month = {09},
pages = {544-559},
title = {Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II},
isbn = {978-3-030-01215-1},
doi = {10.1007/978-3-030-01216-8_33}
}

@misc{MonoDepth2,
      title={Digging Into Self-Supervised Monocular Depth Estimation}, 
      author={Clément Godard and Oisin Mac Aodha and Michael Firman and Gabriel Brostow},
      year={2019},
      eprint={1806.01260},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1806.01260}, 
}

@misc{depthanythingv2,
      title={Depth Anything V2}, 
      author={Lihe Yang and Bingyi Kang and Zilong Huang and Zhen Zhao and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},
      year={2024},
      eprint={2406.09414},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09414}, 
}

@misc{depthpro,
      title={Depth Pro: Sharp Monocular Metric Depth in Less Than a Second}, 
      author={Aleksei Bochkovskii and Amaël Delaunoy and Hugo Germain and Marcel Santos and Yichao Zhou and Stephan R. Richter and Vladlen Koltun},
      year={2024},
      eprint={2410.02073},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.02073}, 
}

@article{DenseDepth,
   title={Dense depth estimation from multiple 360-degree images using virtual depth},
   volume={52},
   ISSN={1573-7497},
   url={http://dx.doi.org/10.1007/s10489-022-03391-w},
   DOI={10.1007/s10489-022-03391-w},
   number={12},
   journal={Applied Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Yang, Seongyeop and Kim, Kunhee and Lee, Yeejin},
   year={2022},
   month=mar, pages={14507–14517} }


